{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraint-based structual learning for Bayesian Network\n",
    "* This is an **Interactive Version**, which unpacks functions for interactive use. For a compact version, please refer to **main.py** or **constraintBN.ipynb**\n",
    "* The algorithm are implemented based on [Scutari](https://arxiv.org/pdf/1406.7648.pdf)\n",
    "* The algorithm will run several iterations. Each time, it will go through four main stages: \n",
    "    * sampling & preprocessing\n",
    "    * finding Markov Blankets\n",
    "    * determining Neighbors\n",
    "    * learning arc directions\n",
    "* Attributes and Edges will be returned.\n",
    "* For interactive purpose, this file will go through one iteration step by step, and it will perform given iterations at the final stage to show the result.\n",
    "\n",
    "### Load Data & Specifying Parameters\n",
    "* The data needs to be .csv files and we need to replace all the \",\" within the cell before processing\n",
    "* required **filename** and **location** of the dataset, optional parameters are **sample_size**, number of iterations (**iteration**), and **alpha** for independence tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from copy import copy\n",
    "import math\n",
    "import csv\n",
    "\n",
    "filename = '500_Cities__Local_Data_for_Better_Health__2017_release.csv'\n",
    "location = '../datasets/'\n",
    "sample_size = 100\n",
    "iteration = 5\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling & Preprocessing\n",
    "* **reformat**: it uniformly randomly selected [sample_size] records from the dataset and print out the attributes names and their indexes\n",
    "* **replace_str**: the records of a given dataset will be transformed into numbers for further computing. For example, if a column has [a,b,b,c,d,c], it will become [0,1,1,2,3,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from utility.py\n",
    "'''\n",
    "def replace_str(data, return_dic = False):\n",
    "    i = 0\n",
    "    value_dic = {}\n",
    "    for col in range(len(data[0])):\n",
    "        unique = {}\n",
    "        index = 0\n",
    "        t = 0\n",
    "        for row in data:\n",
    "            if row[col] not in unique.keys():\n",
    "                unique[row[col]] = index\n",
    "                row[col] = index\n",
    "                index+=1\n",
    "            else:\n",
    "                row[col] = unique[row[col]]\n",
    "        value_dic[col] = unique\n",
    "    if return_dic:\n",
    "        return data, value_dic\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def reformat(path, clean_path = \"\", size = 1000):\n",
    "    with open(path) as csvfile:\n",
    "        raw = csvfile.readlines()\n",
    "        fieldnames = raw[0].strip('\\n').split(\",\")\n",
    "    raw = raw[1:]\n",
    "    sample = np.random.choice(len(raw), size)\n",
    "    sample = sample.tolist()\n",
    "    split_raw = []\n",
    "    for i in sample:\n",
    "        row = raw[i].split(\",\")\n",
    "        split_raw.append(row)\n",
    "    numeric_raw = replace_str(split_raw)\n",
    "    return numeric_raw, fieldnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Markov Blankets\n",
    "* To find the markov blankets, I mainly used Grow and Shrink Algorithm from [Margaritis's Thesis](https://www.cs.cmu.edu/~dmarg/Papers/PhD-Thesis-Margaritis.pdf).\n",
    "    * Grow Phase: \n",
    "      While $\\exists(Y)\\in U -{X}$ such that $Y not\\perp X|S $ do $S\\gets S\\cup{Y}$.\n",
    "    * Shrink Phase:\n",
    "      While $\\exists(Y)\\in S $ such that $Y \\perp X|S-{Y} $ do $S\\gets S-{Y}$.\n",
    "* After finding all the MB, I will perform a symmetric check (when x belongs to y's blanket, if y belongs to x's blanket) and drop out those that do not hold to reduce false postives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utility import *\n",
    "'''\n",
    "from learnAlg.py: learn Markov Blanket Using GS\n",
    "'''\n",
    "def gs(data, alpha):\n",
    "    # number of attributes\n",
    "    n_attr = data.shape[1]\n",
    "    # number of records\n",
    "    n_rec = data.shape[0]\n",
    "    col_index = range(n_attr)\n",
    "    # init empty blanket container for each attri\n",
    "    blanket = dict([(i,[]) for i in range(n_attr)])\n",
    "    for X in col_index:\n",
    "        # step 1: init blanket for attri\n",
    "        S = []\n",
    "        # step2: GROWING phase\n",
    "        for Y in col_index:\n",
    "            # exists Y not belonging to X nor S\n",
    "            if X != Y and Y not in S:\n",
    "                columns = (X,Y) + tuple(S)\n",
    "                if not are_independent(data[:,columns]):\n",
    "                    S.append(Y)\n",
    "        # step3: SHRINKING phase\n",
    "        for Y in S:\n",
    "            # test if Y == X \n",
    "            if X != Y:\n",
    "                new_S = copy(S)\n",
    "                new_S.remove(Y)\n",
    "                columns = (X,Y) + tuple(new_S)\n",
    "                # Y indep of X given S - Y, S = S - Y\n",
    "                if are_independent(data[:,columns]):\n",
    "                    S = new_S\n",
    "        # save to blanket\n",
    "        blanket[X] = S\n",
    "    return blanket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independence Test\n",
    "* In Grow and Shrink Algorithm, we used **are_independent** and **alpha** to test the independence or conditional independence of X and Y.\n",
    "* The independence tests are developed based on Daphne's book in Chapter 18.2.2, page 789-790 -- Using chi-sqaure to calculate the deviance and p-value, then comparing the p-value with a threshold of alpha (default: 0.05).\n",
    "* Notes:\n",
    "    * Here I used 1e-7 to avoid division by zero.\n",
    "    * If there are more than 3 columns passed, I will concatenate all the columns after the second column to be a single Z. In this way, the dimension of computing is always <= 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from utility.py:\n",
    "'''\n",
    "def are_independent(data, alpha = 0.05):\n",
    "    pval = indep_test(data)\n",
    "    if pval < alpha:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "'''\n",
    "Independent tests:\n",
    "@param test: perform chi-square test\n",
    "For data = [X,Y]\n",
    "- calculate joint prob\n",
    "- calculate marginal prob\n",
    "- cross product of marginal X and marginal Y\n",
    "- calculate chi2 statistics\n",
    "'''\n",
    "def indep_test(data, test=True):\n",
    "    bins = unique_bins(data)\n",
    "    n_row = data.shape[0]\n",
    "    if len(bins) == 2:\n",
    "        # PAGE 788-789\n",
    "        # frequency counts\n",
    "        hist,_ = np.histogramdd(data, bins=bins[0:2]) \n",
    "        # joint probability distribution over X,Y,(Z)\n",
    "        Pxy = hist / data.shape[0] \n",
    "        # marginal: axis 0: combine rows/across X; axis 1: combine cols/across Y\n",
    "        Px = np.sum(Pxy, axis = 1) # P(X,Z)\n",
    "        Py = np.sum(Pxy, axis = 0) # P(Y,Z) \n",
    "        # avoid division by zero\n",
    "        Px += 1e-7\n",
    "        Py += 1e-7\n",
    "        # deviance using chi-square\n",
    "        chi = 0\n",
    "        for i in range(bins[0]):\n",
    "            for j in range(bins[1]):\n",
    "                chi += n_row * math.pow(Pxy[i][j] - Px[i] * Py[j], 2) / Px[i] * Py[j]\n",
    "        dof = (bins[0] - 1) * (bins[1] - 1)\n",
    "        p_val = 2*stats.chi2.pdf(chi, dof) # 2* for one tail\n",
    "        return round(p_val,4)\n",
    "    \n",
    "    else:\n",
    "        # PAGE 790, condition on Z\n",
    "        # CHECK FOR > 3 COLUMNS -> concatenate Z into one column\n",
    "        if len(bins) > 3:\n",
    "            data = data.astype('str')\n",
    "            ncols = len(bins)\n",
    "            for i in range(len(data)):\n",
    "                data[i,2] = ''.join(data[i,2:ncols])\n",
    "            data = data.astype('int')[:,0:3]\n",
    "\n",
    "        bins = unique_bins(data)\n",
    "        hist,_ = np.histogramdd(data, bins=bins)\n",
    "\n",
    "        # joint probability distribution over X,Y,Z\n",
    "        Pxyz = hist / n_row\n",
    "        Pz = np.sum(Pxyz, axis = (0,1)) # P(Z)\n",
    "        Pxz = np.sum(Pxyz, axis = 1) # P(X,Z)\n",
    "        Pyz = np.sum(Pxyz, axis = 0) # P(Y,Z)   \n",
    "\n",
    "        Pxy_z = Pxyz / (Pz+1e-7) # P(X,Y | Z) = P(X,Y,Z) / P(Z)\n",
    "        Px_z = Pxz / (Pz+1e-7) # P(X | Z) = P(X,Z) / P(Z)   \n",
    "        Py_z = Pyz / (Pz+1e-7) # P(Y | Z) = P(Y,Z) / P(Z)\n",
    "\n",
    "        Px_y_z = np.empty((Pxy_z.shape)) # P(X|Z)P(Y|Z)\n",
    "        \n",
    "        # avoid division by zero\n",
    "        Pz += 1e-7\n",
    "        \n",
    "        # (M[x,y,z] - M*P(z)P(x|z)P(y|z))^2 / M * P(z)P(x|z)P(y|z)\n",
    "        chi = 0\n",
    "        for i in range(bins[0]):\n",
    "            for j in range(bins[1]):\n",
    "                for k in range(bins[2]):\n",
    "                    Px_y_z[i][j][k] = Px_z[i][k]*Py_z[j][k] + 1e-7\n",
    "                    chi += n_row * math.pow((Pxyz[i][j][k] - Pz[k] * Px_y_z[i][j][k]), 2) / (Pz[k] * Px_y_z[i][j][k])\n",
    "        dof = (bins[0] - 1) * (bins[1] - 1) * bins[2]\n",
    "        p_val = 2*stats.chi2.pdf(chi, dof) # 2* for one tail\n",
    "        \n",
    "        return round(p_val,4)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symmetric Check\n",
    "* The step is used to reduce false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from learnAlg.py: check symmetric for mb or nb\n",
    "'''\n",
    "def check_symmetric(mb):\n",
    "    new_mb = dict(mb)\n",
    "    attr = mb.keys()\n",
    "    for x in attr:\n",
    "        for i in mb[x]:\n",
    "            if x not in mb[i]:\n",
    "                new_mb[x].remove(i)\n",
    "    return new_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Neighbors\n",
    "* For each pair of attribute X and Y, where X is not the same as Y, search for a set (including empty set) on which X and Y are independent. If no such set exists, place an undirected arc between X and Y.\n",
    "* In this step, I used MB to reduce the search space. Specifically:\n",
    "    * if X in Y's MB:\n",
    "        * search all the subsets of MB(Y). once found a subset seperating X and Y -> they are not neighbors\n",
    "        * if no such set found, test independence of X and Y without conditions\n",
    "        * if still not independent, add Y to X's neighbor\n",
    "    * if X not in Y's MB:\n",
    "        * given MB(Y), X and Y must be independent -> they are not neighbors\n",
    "* check symmetric again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from learnAlg.py: learn neighbours\n",
    "'''\n",
    "def learnNb(data, mb, alpha):\n",
    "    nb = {}\n",
    "    # N(x) is subset of B(x)\n",
    "    for x in range(data.shape[1]):\n",
    "        nb[x] = []\n",
    "        for y in range(data.shape[1]):\n",
    "            if x in mb[y]:\n",
    "                space = copy(mb[y]).remove(x)\n",
    "                noset = True\n",
    "                if space != None:\n",
    "                    subset = find_subset(space)\n",
    "                    for s in subset.values():\n",
    "                        columns = (x,y,s)\n",
    "                        if are_independent(data[:,columns]):\n",
    "                            noset = False\n",
    "                            break\n",
    "                # test empty s\n",
    "                columns = (x,y)\n",
    "                if are_independent(data[:,columns]):\n",
    "                    noset = False\n",
    "                if noset:\n",
    "                    nb[x].append(y)\n",
    "                    # place an undirected edge beteewn x and y\n",
    "                    #print \"{} and {} has an edge\".format(x, y)\n",
    "    return check_symmetric(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Arc Directions\n",
    "* 1) Learn V-structure. For each non-adjacent X,Y with a common neighbor S, check if given Z, X and Y are independent. If not, create a v structure with {X -> S <- Y}\n",
    "* 2) After learning v-structures, recursively check two rules:\n",
    "    * If X - Y and there is a directed path from X to Y, then change X-Y to X->Y\n",
    "    * If X, Y are not adjacent, check if an S exists such that X -> S, S - Y, then change S - Y to S -> Y.\n",
    "* also referenced Chap 3.3-3.4 in Daphne's book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from learnAlg.py: learn arc directions\n",
    "'''\n",
    "def learnDir(data, nb, alpha):\n",
    "    leftToRight = {}\n",
    "    # find V-structure\n",
    "    for x in nb.keys():\n",
    "        leftToRight[x] = []\n",
    "        for y in range(x+1, data.shape[1]):\n",
    "            # find non-adjacent x,y\n",
    "            if y in nb[x]:\n",
    "                continue\n",
    "            # find their common neighbor\n",
    "            commonNb = list(set(nb[x]).intersection(nb[y]))\n",
    "            for s in commonNb:\n",
    "                # check if x and y are independent given common neighbour belongs\n",
    "                columns = (x,y,s)\n",
    "                if not are_independent(data[:,columns]):\n",
    "                    if s not in leftToRight[x]:\n",
    "                        leftToRight[x].append(s)\n",
    "                    if y not in leftToRight.keys():\n",
    "                        leftToRight[y] = []\n",
    "                    if s not in leftToRight[y]:\n",
    "                        leftToRight[y].append(s)\n",
    "                    #print \"{} -> {} <- {}\".format(x, s, y)\n",
    "    # recursively applying two rules util converge\n",
    "    last = {}\n",
    "    while last != leftToRight:\n",
    "        last = copy(leftToRight)\n",
    "        for x in nb.keys():\n",
    "            for y in nb.keys():\n",
    "                # case1: adjacent\n",
    "                if y in nb[x]:\n",
    "                    # find undirected edges\n",
    "                    if y in leftToRight[x] or x in leftToRight[y]:\n",
    "                        continue\n",
    "                    # if existing a directed path from x to y, set x -> y\n",
    "                    if hasDirectedPath(x,y,leftToRight):\n",
    "                        if y not in leftToRight[x]:\n",
    "                            leftToRight[x].append(y)\n",
    "                        #print \"{} -> {}\".format(x, y)\n",
    "\n",
    "                # case2: non-adjacent\n",
    "                # if existing s that x -> s and s - y. set s -> y\n",
    "                else:\n",
    "                    for s in leftToRight[x]:\n",
    "                        if s in nb[y]:\n",
    "                            # not s <- y\n",
    "                            if y not in leftToRight[s] and s not in leftToRight[y]:\n",
    "                                leftToRight[s].append(y)\n",
    "                            #print \"{} -> {}\".format(s, y)\n",
    "    return leftToRight\n",
    "\n",
    "'''\n",
    "recursive call to find a directed path from x to y\n",
    "'''\n",
    "def hasDirectedPath(x, y, leftToRight):\n",
    "    if leftToRight[x] == None:\n",
    "        return False\n",
    "    if y in leftToRight[x]:\n",
    "        return True\n",
    "    else:\n",
    "        for i in leftToRight[x]:\n",
    "            if hasDirectedPath(i, y, leftToRight):\n",
    "                return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterations and Count Occurences of Each Edge\n",
    "* After all iterations are done, **edges** are returned in a form like {left node:{right node: # occurences in all iterations}} or you could use **printEdge** to print the edges exceeding a specified **threshold**.\n",
    "* Example for threshold: if edge x -> y appeared 6 times in 10 iterations, with a threshold of 0.5, since 6 > 10*0.5, the edge will be displayed by the printEdge function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def constraintBN(filename, location, sample_size = 100, iteration = 1, alpha = 0.5):\n",
    "    # left -> right\n",
    "    edge = {}\n",
    "    print \"sample_size: {}, iterations: {}, alpha: {}\".format(sample_size, iteration, alpha)\n",
    "    for i in range(iteration):\n",
    "        print \"iteration {}\".format(i)\n",
    "        path = location + filename\n",
    "        #clean_path = location + filename.split(\".\")[0] + \"-num.csv\"\n",
    "        # Reformat data and replace string, size = sample_size\n",
    "        data, field = reformat(path, size = sample_size)\n",
    "        if i == 0:\n",
    "            printAttr(field)\n",
    "        data = np.array(data, np.int32)\n",
    "        #data = np.genfromtxt(clean_path, dtype='int32', delimiter=',')\n",
    "        # 1. Find Markov Blankets\n",
    "        mb = gs(data, alpha = alpha)\n",
    "        mb = check_symmetric(mb)\n",
    "        # 2. learning neighbors\n",
    "        nb = learnNb(data, mb, alpha = alpha)\n",
    "        nb = check_symmetric(nb)\n",
    "        # 3. learning directions\n",
    "        arc = learnDir(data, nb, alpha = alpha)\n",
    "        # calculate occurences\n",
    "        for left in arc.keys():\n",
    "            right = arc[left]\n",
    "            if left not in edge.keys():\n",
    "                edge[left] = {}\n",
    "            for r in right:\n",
    "                if r not in edge[left].keys():\n",
    "                    edge[left][r] = 1\n",
    "                else:\n",
    "                    edge[left][r] += 1\n",
    "    printEdge(edge, itr = iteration)\n",
    "    return edge\n",
    "\n",
    "def printEdge(edge, itr, threshold = 0.8):\n",
    "    for e in edge:\n",
    "        right = edge[e]\n",
    "        for r in right:\n",
    "            if edge[e][r] > threshold*itr:\n",
    "                print \"{} -> {} ({})\".format(e, r, edge[e][r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_size: 100, iterations: 5, alpha: 0.5\n",
      "iteration 0\n",
      "Attributes - Index\n",
      "Year - 0\n",
      "StateAbbr - 1\n",
      "StateDesc - 2\n",
      "CityName - 3\n",
      "GeographicLevel - 4\n",
      "DataSource - 5\n",
      "Category - 6\n",
      "UniqueID - 7\n",
      "Measure - 8\n",
      "Data_Value_Unit - 9\n",
      "DataValueTypeID - 10\n",
      "Data_Value_Type - 11\n",
      "Data_Value - 12\n",
      "Low_Confidence_Limit - 13\n",
      "High_Confidence_Limit - 14\n",
      "Data_Value_Footnote_Symbol - 15\n",
      "Data_Value_Footnote - 16\n",
      "PopulationCount - 17\n",
      "GeoLocation - 18\n",
      "CategoryID - 19\n",
      "MeasureId - 20\n",
      "CityFIPS - 21\n",
      "TractFIPS - 22\n",
      " - 23_Question_Text\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "0 -> 5 (5)\n",
      "0 -> 9 (5)\n",
      "4 -> 5 (5)\n",
      "4 -> 9 (5)\n",
      "5 -> 13 (5)\n",
      "5 -> 14 (5)\n",
      "5 -> 17 (5)\n",
      "5 -> 18 (5)\n",
      "5 -> 19 (5)\n",
      "5 -> 20 (5)\n",
      "5 -> 21 (5)\n",
      "5 -> 22 (5)\n",
      "5 -> 23 (5)\n",
      "7 -> 5 (5)\n",
      "7 -> 9 (5)\n",
      "9 -> 13 (5)\n",
      "9 -> 14 (5)\n",
      "9 -> 17 (5)\n",
      "9 -> 18 (5)\n",
      "9 -> 19 (5)\n",
      "9 -> 20 (5)\n",
      "9 -> 21 (5)\n",
      "9 -> 22 (5)\n",
      "9 -> 23 (5)\n"
     ]
    }
   ],
   "source": [
    "edges = constraintBN(filename, location, sample_size = sample_size, iteration = iteration, alpha = alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "#### Limited Representation\n",
    "* BN can only be used to represent causal relationship and fail to represnet correlations.\n",
    "* Fail to represent distributions like {A $\\perp$ C | B, D} and {B $\\perp$ D | A, C}\n",
    "\n",
    "#### Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
